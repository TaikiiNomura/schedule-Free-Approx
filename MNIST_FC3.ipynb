{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f3391c",
   "metadata": {},
   "source": [
    "### ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740623bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/User/Documents/GitHub/schedule-Free-Approx/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bin.build_optimizer import BuildOptimizer\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ccd31",
   "metadata": {},
   "source": [
    "### ネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd8aeb",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa76ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data(BS, data_seed, num_workers):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), # MNISTの平均\n",
    "         (0.3081,))                     # 標準偏差\n",
    "        ])\n",
    "\n",
    "    # 乱数生成器の設定\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(data_seed)\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "        )\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        transform=transform\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BS,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        generator = generator\n",
    "        )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BS,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        generator = generator\n",
    "        )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b3acc",
   "metadata": {},
   "source": [
    "### 学習テスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6ffb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_train(\n",
    "        model,\n",
    "        X,\n",
    "        T,\n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        sched_name, \n",
    "        scheduler, \n",
    "        ):\n",
    "    \"\"\"\n",
    "        1 iteration の処理\n",
    "    \"\"\"\n",
    "    \n",
    "    X, T = X.to(device), T.to(device) # データ、教師信号をデバイスに移動\n",
    "    \n",
    "    optimizer.zero_grad()   # 勾配のリセット\n",
    "    Y = model(X)            # 順伝番\n",
    "    loss = criterion(Y, T)  # 損失関数\n",
    "    loss.backward()         # 逆伝番\n",
    "    optimizer.step()\n",
    "\n",
    "    # スケジューラ更新\n",
    "    if scheduler is not None and (\"OneCycle\" in sched_name or \"WarmupCosine\" in sched_name):\n",
    "        scheduler.step()\n",
    "    \n",
    "    \"\"\"\n",
    "        loss.item: イテレーションの平均損失(torch MSEの戻り値はmean reduction)\n",
    "    \"\"\"\n",
    "    iter_loss = loss.item()                                        # イテレーションの合計損失\n",
    "    iter_pred = Y.argmax(dim=1, keepdim=True)\n",
    "    iter_correct = iter_pred.eq(T.view_as(iter_pred)).sum().item() # 正解率の計算\n",
    "    iter_total = T.size(0)                                         # バッチサイズ\n",
    "    current_lrs = optimizer.param_groups[0][\"lr\"]            # 学習率\n",
    "    \n",
    "    return iter_loss, iter_correct, iter_total, current_lrs\n",
    "\n",
    "def train_epoch(\n",
    "        model, \n",
    "        device, \n",
    "        train_loader, \n",
    "        optimizer,\n",
    "        sched_name, \n",
    "        criterion,\n",
    "        scheduler, \n",
    "        ):\n",
    "    \"\"\"\n",
    "        1 epoch の処理\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    if hasattr(optimizer, 'train'):\n",
    "        optimizer.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    lrs_in_epoch = []   # イテレーションごとの学習率を保存\n",
    "\n",
    "    for idx, (X, T) in enumerate(train_loader):\n",
    "        iter_loss, iter_correct, iter_total, current_lrs = iter_train(\n",
    "            model,\n",
    "            X,\n",
    "            T,\n",
    "            device, \n",
    "            optimizer, \n",
    "            criterion, \n",
    "            sched_name, \n",
    "            scheduler, \n",
    "            )\n",
    "        \n",
    "        epoch_loss += iter_loss\n",
    "        epoch_correct += iter_correct\n",
    "        epoch_total += iter_total\n",
    "        lrs_in_epoch.append(current_lrs)\n",
    "    \n",
    "    if scheduler is not None and not (\"OneCycle\" in sched_name or \"WarmupCosine\" in sched_name):\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    epoch_accuracy = 100. * epoch_correct / epoch_total\n",
    "    current_epoch_lrs = optimizer.param_groups[0][\"lr\"] # 学習率\n",
    "\n",
    "    return avg_epoch_loss, epoch_accuracy, lrs_in_epoch, current_epoch_lrs\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b913fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iter(\n",
    "        model, \n",
    "        X, \n",
    "        T, \n",
    "        device, \n",
    "        criterion,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    1イテレーションごとの評価処理\n",
    "    \"\"\"\n",
    "    X, T = X.to(device), T.to(device)\n",
    "    with torch.no_grad():\n",
    "        Y = model(X)\n",
    "        loss = criterion(Y, T)\n",
    "\n",
    "        iter_loss = loss.item()\n",
    "        iter_pred = Y.argmax(dim=1, keepdim=True)\n",
    "        iter_correct = iter_pred.eq(T.view_as(iter_pred)).sum().item()\n",
    "        iter_total = T.size(0)   # ←バッチサイズだけカウント\n",
    "\n",
    "    return iter_loss, iter_correct, iter_total\n",
    "\n",
    "\n",
    "def test_epoch(\n",
    "        model, \n",
    "        optimizer, \n",
    "        test_loader, \n",
    "        device, \n",
    "        criterion\n",
    "        ):\n",
    "    \"\"\"\n",
    "    エポック単位の評価処理\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if hasattr(optimizer, 'eval'):\n",
    "        optimizer.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "\n",
    "    for X, T in test_loader:\n",
    "        iter_loss, iter_correct, iter_total, = test_iter(\n",
    "            model,\n",
    "            X, \n",
    "            T, \n",
    "            device, \n",
    "            criterion,\n",
    "        )\n",
    "        epoch_loss += iter_loss\n",
    "        epoch_correct += iter_correct\n",
    "        epoch_total += iter_total\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(test_loader)\n",
    "    epoch_accuracy = 100.0 * epoch_correct / epoch_total\n",
    "\n",
    "    return avg_epoch_loss, epoch_accuracy\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e00612",
   "metadata": {},
   "source": [
    "### メインループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5f2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "optim: ADINA, loss: [2.311305809020996, 0.23142045736312866, 0.1398782342672348]\n",
      "optim: Schedule-Free ADINA, loss: [2.311305809020996, 0.42273988723754885, 0.32719685733318327]\n",
      "optim: Schedule-Free ADINA Approx, loss: [2.311305809020996, 0.3979625940322876, 0.2816184818744659]\n",
      "optim: Schedule-Free ADINA LRC, loss: [2.311305809020996, 0.2537445604801178, 0.16134749799966813]\n",
      "optim: ADINA, loss: [2.3114015827178953, 0.25982018012059854, 0.27037502286969683]\n",
      "optim: Schedule-Free ADINA, loss: [2.3114015827178953, 0.129552787435567, 0.11341347621844325]\n",
      "optim: Schedule-Free ADINA Approx, loss: [2.3114015827178953, 0.10233302795784548, 0.07733841455800575]\n",
      "optim: Schedule-Free ADINA LRC, loss: [2.3114015827178953, 0.12011071689804084, 0.10215610401436788]\n"
     ]
    }
   ],
   "source": [
    "LRs = [0.001]\n",
    "BSs = [2048, 16]\n",
    "EPOCHS = 2\n",
    "num_workers = 8\n",
    "model_seed = 43\n",
    "data_seed = 43\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Net()\n",
    "pth_name = f'initial_weights_FC_seed={model_seed}.pth'\n",
    "torch.save(model.state_dict(), pth_name)\n",
    "\n",
    "for BS in BSs:\n",
    "\n",
    "    results = []  # 各バッチサイズごとの、すべての最適化手法の結果を保存\n",
    "\n",
    "    for LR in LRs:\n",
    "        \n",
    "        train_loader, test_loader = set_data(BS, data_seed, num_workers)\n",
    "        steps_per_epoch = len(train_loader)  # 1エポックあたりのステップ数\n",
    "\n",
    "        # 各最適化手法ごとにループ\n",
    "        for optm_name in [\n",
    "            # \"Adam\", \n",
    "            # \"Adam+StepLR\", \n",
    "            # \"Adam+MultiStepLR\",\n",
    "            # \"Adam+ExponentialLR\", \n",
    "            # \"Adam+CosineAnnealing\",\n",
    "            # \"Adam+OneCycleLR\", \n",
    "            # \"Adam+WarmupCosine\",\n",
    "            # \"Schedule-Free Adam\",\n",
    "            \"ADINA\",\n",
    "            # \"ADINA+StepLR\", \n",
    "            # \"ADINA+MultiStepLR\",\n",
    "            # \"ADINA+ExponentialLR\", \n",
    "            # \"ADINA+CosineAnnealing\",\n",
    "            # \"ADINA+OneCycleLR\", \n",
    "            # \"ADINA+WarmupCosine\",\n",
    "            \"Schedule-Free ADINA\",\n",
    "            \"Schedule-Free ADINA Approx\",\n",
    "            \"Schedule-Free ADINA LRC\",\n",
    "            ]:\n",
    "\n",
    "            # モデルインスタンスを生成\n",
    "            model = Net()\n",
    "            model.load_state_dict(torch.load(pth_name))\n",
    "            model.to(device)\n",
    "\n",
    "            # モデルに対応した optimizer/scheduler を構築\n",
    "            builder = BuildOptimizer(\n",
    "                model,\n",
    "                lr=LR,\n",
    "                total_epochs=EPOCHS,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                )\n",
    "            optimizer, scheduler, sched_name = builder.build_optimizers_and_schedulers()[optm_name]\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # 各手法の訓練損失、訓練精度、テスト損失、テスト精度, 学習率を保存する配列\n",
    "            train_losses, train_accuracies = [], []\n",
    "            test_losses, test_accuracies = [], []\n",
    "            train_epoch_lrs, train_iter_lrs = [], []\n",
    "\n",
    "            # --- ０回目のテストケース計算 ---\n",
    "            epoch_test_loss, epoch_test_accuracy = test_epoch(\n",
    "                model,\n",
    "                optimizer,\n",
    "                test_loader,\n",
    "                device,\n",
    "                criterion,\n",
    "            )\n",
    "            test_losses.append(epoch_test_loss)\n",
    "            test_accuracies.append(epoch_test_accuracy)\n",
    "\n",
    "            # 学習\n",
    "            for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "                # --- train ---\n",
    "                epoch_train_loss, epoch_train_accuracy, lrs_in_epoch, current_epoch_lrs = train_epoch(\n",
    "                    model,\n",
    "                    device,\n",
    "                    train_loader,\n",
    "                    optimizer,\n",
    "                    sched_name,\n",
    "                    criterion,\n",
    "                    scheduler,\n",
    "                )\n",
    "                train_losses.append(epoch_train_loss)\n",
    "                train_accuracies.append(epoch_train_accuracy)\n",
    "                train_epoch_lrs.append(current_epoch_lrs)\n",
    "                train_iter_lrs.append(lrs_in_epoch)\n",
    "\n",
    "                # --- test ---\n",
    "                epoch_test_loss, epoch_test_accuracy = test_epoch(\n",
    "                    model,\n",
    "                    optimizer,\n",
    "                    test_loader,\n",
    "                    device,\n",
    "                    criterion,\n",
    "                )\n",
    "                test_losses.append(epoch_test_loss)\n",
    "                test_accuracies.append(epoch_test_accuracy)\n",
    "                \n",
    "            result_entry = {\n",
    "                'seed': model_seed,\n",
    "                'batch_size': BS,\n",
    "                'learning_rate': LR,\n",
    "                'optimizer': optm_name,\n",
    "                'train_losses': train_losses,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'test_losses': test_losses,\n",
    "                'test_accuracies': test_accuracies,\n",
    "                'epoch_lrs': train_epoch_lrs,\n",
    "                'iter_lrs': train_iter_lrs,\n",
    "            }\n",
    "\n",
    "            print(f'optim: {optm_name}, loss: {test_losses}')\n",
    "\n",
    "            results.append(result_entry)\n",
    "\n",
    "    # 各バッチサイズごとの結果をJSONへ保存\n",
    "    filename = f'results_FC3_BS={BS}_seed={model_seed}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32733ce3",
   "metadata": {},
   "source": [
    "### 結果の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abd1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4101a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x, y, color='r')   # red\n",
    "# plt.plot(x, y, color='b')   # blue\n",
    "# plt.plot(x, y, color='g')   # green\n",
    "# plt.plot(x, y, color='k')   # black\n",
    "# plt.plot(x, y, color='c')   # cyan\n",
    "# plt.plot(x, y, color='m')   # magenta\n",
    "# plt.plot(x, y, color='y')   # yellow\n",
    "# plt.plot(x, y, color='w')   # white\n",
    "\n",
    "# スタイル設定\n",
    "style_map = {\n",
    "    'Adam': {'color': 'blue', 'linestyle': '-'},\n",
    "    'Schedule-Free Adam': {'color': 'red', 'linestyle': '-'},\n",
    "    'Schedule-Free AdamW closure': {'color': 'blue', 'linestyle': '-'},\n",
    "    'Adam+StepLR': {'color': 'g', 'linestyle': '-'},\n",
    "    'Adam+MultiStepLR': {'color': 'k', 'linestyle': '-'},\n",
    "    'Adam+ExponentialLR': {'color': 'c', 'linestyle': '-'},\n",
    "    'Adam+CosineAnnealing': {'color': 'm', 'linestyle': '-'},\n",
    "    'Adam+OneCycleLR': {'color': 'y', 'linestyle': '-'},\n",
    "    'Adam+WarmupCosine': {'color': 'w', 'linestyle': '-'},\n",
    "    \n",
    "    'ADINA': {'color': 'b', 'linestyle': '--'},\n",
    "    'Schedule-Free ADINA': {'color': 'b', 'linestyle': '-'},\n",
    "    'Schedule-Free ADINA(λ=0)': {'color': 'orange', 'linestyle': '-.'},\n",
    "    'Schedule-Free ADINA(λ=1)': {'color': 'pink', 'linestyle': '-'},\n",
    "    'Schedule-Free ADINA Approx': {'color': 'r', 'linestyle': '--'},\n",
    "    'Schedule-Free ADINA LRC': {'color': 'r', 'linestyle': '-'},\n",
    "    'ADINA+StepLR': {'color': 'g', 'linestyle': '-'},\n",
    "    'ADINA+MultiStepLR': {'color': 'k', 'linestyle': '-'},\n",
    "    'ADINA+ExponentialLR': {'color': 'c', 'linestyle': '-'},\n",
    "    'ADINA+CosineAnnealing': {'color': 'm', 'linestyle': '-'},\n",
    "    'ADINA+OneCycleLR': {'color': 'y', 'linestyle': '-'},\n",
    "    'ADINA+WarmupCosine': {'color': 'w', 'linestyle': '-'},\n",
    "\n",
    "    # '': {'color': '', 'linestyle': ''},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaeef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BSs = [2048, 1024, 512, 64, 32, 16]\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "BSs = [2048, 16]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "# 凡例用に線の情報を保存しておく\n",
    "legend_handles = []\n",
    "\n",
    "for i, BS in enumerate(BSs):\n",
    "    ax = axes[i]\n",
    "    pass_name = f'/mnt/c/Users/User/Documents/GitHub/schedule-Free-Approx/result/results_FC3_BS={BS}_seed=43.json'\n",
    "    with open(pass_name, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    for entry in results:\n",
    "        optimizer = entry['optimizer']\n",
    "\n",
    "        if optimizer not in style_map:\n",
    "            continue\n",
    "\n",
    "        test_acc = entry['test_accuracies']\n",
    "        epochs = list(range(1, len(test_acc) + 1))\n",
    "        style = style_map[optimizer]\n",
    "\n",
    "        if optimizer == 'Schedule-Free ADINA(λ=0)':\n",
    "            optimizer = 'Polyak-Ruppert Ave. ADINA'\n",
    "        elif optimizer == 'Schedule-Free ADINA(λ=1)':\n",
    "            optimizer = 'Primal Ave. ADINA'\n",
    "\n",
    "        line, = ax.plot(\n",
    "            epochs,\n",
    "            test_acc,\n",
    "            label=optimizer,\n",
    "            color=style['color'],\n",
    "            linestyle=style['linestyle'],\n",
    "            linewidth=style.get('linewidth', 1.5)\n",
    "        )\n",
    "\n",
    "        # 1回目だけ凡例に追加（重複防止）\n",
    "        if i == 0:\n",
    "            legend_handles.append(line)\n",
    "\n",
    "    ax.set_title(f'Batch Size = {BS}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_ylim(80, 100)\n",
    "    ax.grid(True)\n",
    "\n",
    "# 下中央にまとめて凡例を表示\n",
    "fig.legend(handles=legend_handles, loc='lower center', ncol=4, fontsize=10, frameon=False)\n",
    "\n",
    "# 下部を空けて配置調整\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
